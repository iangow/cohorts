# Un intermezzo

I take a brief pause in this chapter to organize some bigger-picture thoughts about Chapter 4 of @tanimura2021sql.

My first thought is that the distinctions between each pair of the four types of cohort analysis identified by @tanimura2021sql [p. 124] are not equally sharp or stark.
For example, the distinction between retention and survivorship appears to turn on whether the activity engaged involves "repeated actions" or not.
While this distinction certainly matters from a data analysis perspective, it's not clear that it's central to the analysis and my inclination is to view survivorship as the more fundamental idea underlying both.

Returnship, the third type of cohort analysis identified by @tanimura2021sql, seems to be one of repeated actions occurring "more than some minimum threshold of times", thus putting it close to retention and thus also falling under survivorship.

Finally, cumulative analyses "are concered with the total number of times or amounts measured at one or more fixed time windows".
In a sense, returnship seems to involve counting over windows, while cumulative analysis likely requires summing of some measure.
Given that a count is simply a sum of ones that indicate an action, this suggests that returnship is often perhaps a species of cumulative analysis.

@tanimura2021sql seems to concur regarding the existence of fundamental links between the four types of analysis, but elects to treat retention as the basic building block.
My inclination is to consider survivorship as the elemental analysis and to build up from there.
My hunch is that a lot of the work in Chapter 4 of @tanimura2021sql can be better understood if one begins each analysis with a step that organizes the cohort data into a canonical structure.


```{r}
#| warning: false
#| include: true
library(DBI)
library(tidyverse)
library(dbplyr)
library(ggplot2)
library(knitr)
```

```{r}
#| include: false
# options("tinytable_tt_digits" = 4)
library(tinytable)

print.tbl_sql <- function(x, ...) {
  collect(x, n = 10) |> 
    tt(...) |>
    knitr::knit_print()
}

registerS3method("print", "tbl_sql", print.tbl_sql)
```


```{r}
#| include: true
db <- dbConnect(duckdb::duckdb())
```

```{r}
#| include: true
load_parquet <- function (conn, table, data_dir = "data") {
    file_path <- file.path(data_dir, paste0(table, ".parquet"))
    df_sql <- paste0("SELECT * FROM read_parquet('", file_path, "')")
    dplyr::tbl(conn, dplyr::sql(df_sql)) 
}
```

```{r}
#| include: true
legislators_terms <- load_parquet(db, "legislators_terms")
legislators <- load_parquet(db, "legislators")
```

## Survival data: The basic building block

@singer2003longitudinal [p. 306] suggest that "researchers use **survival analysis** in a variety of contexts that share a common characterisit: interest centers on describing *whether events occur* or *when events occur*."

Often analytical simplicity is obtained by defining the event in such a way that it can happen at most once.
@singer2003longitudinal provide examples such as time to relapse among recently treated alcoholic and age at first suicide ideation.
While one could track alcoholics over an extended period that allows them the opportunity to relapse, re-enter treatment, and then perhaps relapse again, by defining relapse as the first such event after treatment, it can occur at most once.
In other settings, events occur at most once for each individual (e.g., death for most individual humans).

I suggest that the most basic form of data for survival analysis is a table with the following columns:

 - Identifier: One more columns that identify the individuals in the sample.
 In the legislator data, this would be `id_bioguide`.
 - Entry date: The relevant start date for the individual
 - Event date: The date, if any, on which the relevant event occurs for the individual.
 
Note that we will often not observe event dates for many observations due to **censoring**.
While events may be inevitable for any given individual (the proverbial death and taxes), such events need not occur during the period covered by the sample under study.
This fact may not always be obvious from the data set in question.

To illustrate this point, I consider the legislators data .
It is important to recognize that the data in `legislators` and `legislators_terms` were obtained at some unspecified date in the past.

```{r}
max_term_start <-
  legislators_terms |>
  summarize(max(term_start, na.rm = TRUE)) |>
  pull()

max_term_start
```

As we can see there are no terms in the data *starting* after the November 2020 election, suggesting that the data were extracted before terms related to that election started.
Looking at the `term_end` dates, we see that no member of the House of Representatives (`term_type == "rep"`) has a term ending after 
```{r}
legislators_terms |>
  filter(term_end >= max_term_start) |>
  count(term_type, term_end) |>
  arrange(term_type, term_end)
```

For simplicity, I just assume that the extraction date *was* that date.

```{r}
extract_date <- "2020-11-03"
```

Once we recognize that the data were extracted on `extract_date`, we can consider all `term_end` dates after that date as merely *expected* `term_end` dates, as subsequent events might have led to some members of Congress to finish their terms early.
For this reason, it is arguably more appropriate to code such `term_end` values as "don't know" or `NA`.
This is especially so when we view `last_term` as representing the 

```{r}
survival_data <-
  legislators_terms |>
  group_by(id_bioguide) |>
  summarize(entry_date = min(term_start, na.rm = TRUE),
            exit_date = max(term_end, na.rm = TRUE),
            .groups = "drop") |>
  mutate(first_century = century(entry_date), .before = 1,
         exit_date = if_else(exit_date >= extract_date, 
                             NA, exit_date))

survival_data
```

## Survival data with cohorts: The canonical form

and cohorts formed based on `first_century`, one of the cohort variables studied in @sec-original

- Cohort: One or more columns that identify the cohort to which an individual belongs.





